{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/chu-ise/411A-2022/blob/main/homework/HW02/homework_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHSBsEOJWbYI"
      },
      "source": [
        "# Homework #2\n",
        "\n",
        "> **Due date: Aptril 6, 2022**\n",
        "\n",
        "Complete the following cells by putting your code in the placeholders `[# put your code here]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install demoji contractions unidecode num2words pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZVbMRVz7WbYV",
        "outputId": "f44514fe-2418-4d14-f13c-85cb40264eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - negative_tweets.json\n",
            " - positive_tweets.json\n",
            " - tweets.20150430-223406.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     /Users/yj.lee/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries and load the data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import demoji\n",
        "import contractions\n",
        "import unidecode\n",
        "from num2words import num2words\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "pd.options.display.max_columns=None\n",
        "pd.options.display.max_rows=None\n",
        "pd.options.display.max_colwidth=None\n",
        "\n",
        "nltk.download('twitter_samples') # Download the dataset\n",
        "\n",
        "# We are going to use the Negative and Positive Tweets file which each contains 5000 tweets.\n",
        "for name in twitter_samples.fileids():\n",
        "    print(f' - {name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8Y4Boh0fWbYX"
      },
      "outputs": [],
      "source": [
        "# Load the negative tweets file and assign label as 0 for negative\n",
        "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")\n",
        "df_neg = pd.DataFrame(negative_tweets, columns=['text'])\n",
        "df_neg['label'] = 0\n",
        "\n",
        "# Load the positive tweets file and assign label as 1 for positive\n",
        "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
        "df_pos = pd.DataFrame(positive_tweets, columns=['text'])\n",
        "df_pos['label'] = 1\n",
        "\n",
        "df = pd.concat([df_pos, df_neg]) # Concatenate both the files\n",
        "df = df.sample(frac=1).reset_index(drop=True) # Shuffle the data to mix negative and positive tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "i2Y0MikPWbYY",
        "outputId": "0214d945-7e58-4a00-f648-cc4453074995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the whole data is: 10000 rows and 2 columns\n"
          ]
        }
      ],
      "source": [
        "print(f'Shape of the whole data is: {df.shape[0]} rows and {df.shape[1]} columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5mlDWX4WbYa",
        "outputId": "57f48e06-660d-421f-d24d-0e7cef1eb17a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@AlyssaC_HK I use Chrome :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This manga is just too cute and yet made me cry.. :( http://t.co/KB6GswBxMT</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@joohyunvrl you are a goddess :D</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&amp;amp; why tf can't i find subtitles for OITNB ? I need to understand Changs backstory :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>But you know I do na... :( We can negotiate the Bride Price Advance Payment https://t.co/2uRT9ae8Px</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@Jaaeeeeee lucky :( I was feeling hungry, but then I didn't wanna get out of bed either, so I gotta wait until ma√±ana</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@DiongzonS  follow @jnlazts &amp;amp; http://t.co/RCvcYYO0Iq follow u back :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>i miss watching anna akana videos :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@SupportOrganize Hey Lynne, on FB the Buffer button will bring up your composer and you can Buffer an FB share. :)  -Mary</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@nicoleezzy halaaaang :( just done crying.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                        text  \\\n",
              "0                                                                                                @AlyssaC_HK I use Chrome :)   \n",
              "1                                                This manga is just too cute and yet made me cry.. :( http://t.co/KB6GswBxMT   \n",
              "2                                                                                           @joohyunvrl you are a goddess :D   \n",
              "3                                   &amp; why tf can't i find subtitles for OITNB ? I need to understand Changs backstory :(   \n",
              "4                        But you know I do na... :( We can negotiate the Bride Price Advance Payment https://t.co/2uRT9ae8Px   \n",
              "5      @Jaaeeeeee lucky :( I was feeling hungry, but then I didn't wanna get out of bed either, so I gotta wait until ma√±ana   \n",
              "6                                                  @DiongzonS  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)   \n",
              "7                                                                                       i miss watching anna akana videos :(   \n",
              "8  @SupportOrganize Hey Lynne, on FB the Buffer button will bring up your composer and you can Buffer an FB share. :)  -Mary   \n",
              "9                                                                                 @nicoleezzy halaaaang :( just done crying.   \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      0  \n",
              "2      1  \n",
              "3      0  \n",
              "4      0  \n",
              "5      0  \n",
              "6      1  \n",
              "7      0  \n",
              "8      1  \n",
              "9      0  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look at the head of the dataframe\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAHrfDvDWbYd"
      },
      "source": [
        "## Lower Casing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jZgOzLD4WbYe",
        "outputId": "6f4f1f95-b2c3-4c5e-a685-85a84134cec4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@alyssac_hk i use chrome :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this manga is just too cute and yet made me cry.. :( http://t.co/kb6gswbxmt</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                          text  \\\n",
              "0                                                  @alyssac_hk i use chrome :)   \n",
              "1  this manga is just too cute and yet made me cry.. :( http://t.co/kb6gswbxmt   \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      0  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put your code here\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Dfja2rWbYe"
      },
      "source": [
        "## Remove"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_K7uatdWbYe"
      },
      "source": [
        "### URL's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HLoy603eWbYf",
        "outputId": "b09f5d51-1a9c-4e33-e6be-db462976b162"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@alyssac_hk i use chrome :)</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>this manga is just too cute and yet made me cry.. :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@joohyunvrl you are a goddess :d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&amp;amp; why tf can't i find subtitles for oitnb ? i need to understand changs backstory :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>but you know i do na... :( we can negotiate the bride price advance payment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                       text  \\\n",
              "0                                                               @alyssac_hk i use chrome :)   \n",
              "1                                     this manga is just too cute and yet made me cry.. :(    \n",
              "2                                                          @joohyunvrl you are a goddess :d   \n",
              "3  &amp; why tf can't i find subtitles for oitnb ? i need to understand changs backstory :(   \n",
              "4              but you know i do na... :( we can negotiate the bride price advance payment    \n",
              "\n",
              "   label  \n",
              "0      1  \n",
              "1      0  \n",
              "2      1  \n",
              "3      0  \n",
              "4      0  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put your code here\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smNLnn0jWbYf"
      },
      "source": [
        "### E-mail"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yB8xOUojWbYf",
        "outputId": "91f7d8ee-383c-44cb-d0a4-01b6b9d98936"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I have being trying to contact xyz via email to  but there is no response.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'I have being trying to contact xyz via email to xyz@abc.co.in but there is no response.'\n",
        "# put your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jb-caHxWbYg"
      },
      "source": [
        "### Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "h9DrA1HPWbYg",
        "outputId": "9a936b8e-ca77-4833-ec37-841515b949cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Today is  and after two days on  our vacation starts until '"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Today is 22/12/2020 and after two days on 24-12-2020 our vacation starts until 25th.09.2021\"\n",
        "\n",
        "# 1. Remove date formats like: dd/mm/yy(yy), dd-mm-yy(yy), dd(st|nd|rd).mm/yy(yy)\n",
        "# put your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ITMn5mnoWbYg",
        "outputId": "df57993f-fdf6-46b6-822d-32b00f28de84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Today is  when I am writing this post. I hope to post this byor max to max by or '"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Today is 11th of January, 2021 when I am writing this post. I hope to post this by February 15th or max to max by 20 may 21 or 20th-December-21\"\n",
        "\n",
        "# 2. Remove date formats like: 20 apr 21, April 15th, 11th of April, 2021\n",
        "# put your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilv3oImlWbYg"
      },
      "source": [
        "### HTML Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MAXsf04CWbYh"
      },
      "outputs": [],
      "source": [
        "# Dummy text\n",
        "text = \"\"\"\n",
        "<title>Below is a dummy html code.</title>\n",
        "<body>\n",
        "    <p>All the html opening and closing brackets should be remove.</p>\n",
        "    <a href=\"https://www.abc.com\">Company Site</a>\n",
        "</body>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gRWmFA0_WbYh",
        "outputId": "25215085-197b-49bb-e3fe-b4735fc45577"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nBelow is a dummy html code.\\n\\n    All the html opening and closing brackets should be remove.\\n    Company Site\\n\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Using regex to remove html tags\n",
        "# put your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2_W51jl8WbYh"
      },
      "outputs": [],
      "source": [
        "# Using Beautiful Soup\n",
        "def remove_html(text):\n",
        "    # put your code here\n",
        "\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AcN-6RmzWbYi",
        "outputId": "d4ea1a33-df4c-4f18-a7b5-65839dc4a0cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Below is a dummy html code.\\n\\nAll the html opening and closing brackets should be remove.\\nCompany Site\\n\\n'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_html(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THw3aERFWbYi"
      },
      "source": [
        "### Emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QoHpm0cjWbYi"
      },
      "outputs": [],
      "source": [
        "# Reference: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6_i_2klgWbYi",
        "outputId": "87730c2b-0150-40a9-f92a-3226b64a80eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'game is on . Hilarious'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"game is on üî•üî•. HilariousüòÇ\"\n",
        "remove_emoji(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CiVl-6F4WbYi"
      },
      "outputs": [],
      "source": [
        "# Remove emoji's from text\n",
        "df.text = df.text.apply(lambda x: remove_emoji(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLocYpfWbYi"
      },
      "source": [
        "### Emoticons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tyJoj4WbYi"
      },
      "source": [
        "Emoji's and Emoticons are different. Yes!!<br>\n",
        "Emoticons are used to express facial expressions using keyboard characters such as letters, numbers, and pucntuation marks. Where emjoi's are small images.\n",
        "\n",
        "Thanks to [Neel Shah](https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py) for curating a dictionary of emoticons and their description. We shall use this dictionary and remove the emoticons from our text. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "O92-dgKAWbYi"
      },
      "outputs": [],
      "source": [
        "EMOTICONS = {\n",
        "    u\":‚Äë\\)\":\"Happy face or smiley\",\n",
        "    u\":\\)\":\"Happy face or smiley\",\n",
        "    u\":-\\]\":\"Happy face or smiley\",\n",
        "    u\":\\]\":\"Happy face or smiley\",\n",
        "    u\":-3\":\"Happy face smiley\",\n",
        "    u\":3\":\"Happy face smiley\",\n",
        "    u\":->\":\"Happy face smiley\",\n",
        "    u\":>\":\"Happy face smiley\",\n",
        "    u\"8-\\)\":\"Happy face smiley\",\n",
        "    u\":o\\)\":\"Happy face smiley\",\n",
        "    u\":-\\}\":\"Happy face smiley\",\n",
        "    u\":\\}\":\"Happy face smiley\",\n",
        "    u\":-\\)\":\"Happy face smiley\",\n",
        "    u\":c\\)\":\"Happy face smiley\",\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\n",
        "    u\"=\\]\":\"Happy face smiley\",\n",
        "    u\"=\\)\":\"Happy face smiley\",\n",
        "    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
        "    u\":-\\)\\)\":\"Very happy\",\n",
        "    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\n",
        "    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
        "    u\":'‚Äë\\(\":\"Crying\",\n",
        "    u\":'\\(\":\"Crying\",\n",
        "    u\":'‚Äë\\)\":\"Tears of happiness\",\n",
        "    u\":'\\)\":\"Tears of happiness\",\n",
        "    u\"D‚Äë':\":\"Horror\",\n",
        "    u\"D:<\":\"Disgust\",\n",
        "    u\"D:\":\"Sadness\",\n",
        "    u\"D8\":\"Great dismay\",\n",
        "    u\"D;\":\"Great dismay\",\n",
        "    u\"D=\":\"Great dismay\",\n",
        "    u\"DX\":\"Great dismay\",\n",
        "    u\":‚ÄëO\":\"Surprise\",\n",
        "    u\":O\":\"Surprise\",\n",
        "    u\":‚Äëo\":\"Surprise\",\n",
        "    u\":o\":\"Surprise\",\n",
        "    u\":-0\":\"Shock\",\n",
        "    u\"8‚Äë0\":\"Yawn\",\n",
        "    u\">:O\":\"Yawn\",\n",
        "    u\":-\\*\":\"Kiss\",\n",
        "    u\":\\*\":\"Kiss\",\n",
        "    u\":X\":\"Kiss\",\n",
        "    u\";‚Äë\\)\":\"Wink or smirk\",\n",
        "    u\";\\)\":\"Wink or smirk\",\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\n",
        "    u\";‚Äë\\]\":\"Wink or smirk\",\n",
        "    u\";\\]\":\"Wink or smirk\",\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\n",
        "    u\":‚Äë,\":\"Wink or smirk\",\n",
        "    u\";D\":\"Wink or smirk\",\n",
        "    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
        "    u\":‚Äë\\|\":\"Straight face\",\n",
        "    u\":\\|\":\"Straight face\",\n",
        "    u\":$\":\"Embarrassed or blushing\",\n",
        "    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
        "    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\n",
        "    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
        "    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
        "    u\">:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\">:\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\n",
        "    u\"3:‚Äë\\)\":\"Evil or devilish\",\n",
        "    u\"3:\\)\":\"Evil or devilish\",\n",
        "    u\">;\\)\":\"Evil or devilish\",\n",
        "    u\"\\|;‚Äë\\)\":\"Cool\",\n",
        "    u\"\\|‚ÄëO\":\"Bored\",\n",
        "    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n",
        "    u\"#‚Äë\\)\":\"Party all night\",\n",
        "    u\"%‚Äë\\)\":\"Drunk or confused\",\n",
        "    u\"%\\)\":\"Drunk or confused\",\n",
        "    u\":-###..\":\"Being sick\",\n",
        "    u\":###..\":\"Being sick\",\n",
        "    u\"<:‚Äë\\|\":\"Dump\",\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\n",
        "    u\"\\(';'\\)\":\"Baby\",\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
        "    u\"\\^_\\^\":\"Joyful\",\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
        "    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
        "    u\";_;\":\"Sad or Crying\",\n",
        "    u\";-;\":\"Sad or Crying\",\n",
        "    u\";n;\":\"Sad or Crying\",\n",
        "    u\";;\":\"Sad or Crying\",\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\n",
        "    u\"T\\.T\":\"Sad or Crying\",\n",
        "    u\"QQ\":\"Sad or Crying\",\n",
        "    u\"Q_Q\":\"Sad or Crying\",\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\n",
        "    u\"\\(-_-\\)\":\"Shame\",\n",
        "    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n",
        "    u\"\\(=_=\\)\":\"Tired\",\n",
        "    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n",
        "    u\"=_\\^=\t\":\"cat\",\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
        "    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
        "    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
        "    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
        "    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
        "    u'\\(-\"-\\)':\"Worried\",\n",
        "    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
        "    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
        "    u\":O o_O\":\"Surprised\",\n",
        "    u\"o_0\":\"Surprised\",\n",
        "    u\"o\\.O\":\"Surpised\",\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
        "    u\"oO\":\"Surprised\",\n",
        "    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n",
        "    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QeVE1xA3WbYk"
      },
      "outputs": [],
      "source": [
        "def remove_emoticons(text):\n",
        "    emoticons_pattern = re.compile(u'(' + u'|'.join(emo for emo in EMOTICONS) + u')')\n",
        "    return emoticons_pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5wP2lMC4WbYl",
        "outputId": "508d6581-ab5f-43a2-dd08-1b769bc0ed44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello '"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_emoticons(\"Hello :->\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9s0GURidWbYl"
      },
      "outputs": [],
      "source": [
        "# Remove emoticons from text\n",
        "df.text = df.text.apply(lambda x: remove_emoticons(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX4EDG0NWbYl"
      },
      "source": [
        "### Hashtags and Mentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv5hAKB4WbYm"
      },
      "source": [
        "We are habituated to use hashtags and mentions in our tweet either to indicate the context or bring attention to an individual. Hashtags can be used to extract features, to see what's trending and in various other applications.\n",
        "\n",
        "Since, we don't require them we'll remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6KmbBsswWbYm"
      },
      "outputs": [],
      "source": [
        "def remove_tags_mentions(text):\n",
        "    # put your code here\n",
        "    return pattern.sub('', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "3xcoThItWbYm",
        "outputId": "d36df20f-7ed9-4f31-de09-83e63173d6da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'live  on  - jonah and jareddddd'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"live @flippinginja on #younow - jonah and jareddddd\"\n",
        "remove_tags_mentions(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0BV_kIOaWbYm"
      },
      "outputs": [],
      "source": [
        "# Remove hashtags and mentions\n",
        "df.text = df.text.apply(lambda x: remove_tags_mentions(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsKdXCgRWbYm"
      },
      "source": [
        "### Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "GFrE98nUWbYm"
      },
      "outputs": [],
      "source": [
        "PUNCTUATIONS = string.punctuation\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return # put your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rM_TjQ1pWbYm",
        "outputId": "047b8c72-0f1b-40b7-95cc-44919391111b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' i use chrome ',\n",
              " 'this manga is just too cute and yet made me cry  ',\n",
              " ' you are a goddess d',\n",
              " 'amp why tf cant i find subtitles for oitnb  i need to understand changs backstory ',\n",
              " 'but you know i do na  we can negotiate the bride price advance payment ']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.text = df[\"text\"].apply(lambda text: remove_punctuation(text))\n",
        "df.text[:5].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr85aP8HWbYm"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Qtr2NBfxWbYm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/yj.lee/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    return # put your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "o66K8c81WbYm",
        "outputId": "6eac8b93-c061-496b-eb1b-39c00d4cf7f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['use chrome', 'manga cute yet made cry', 'goddess']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove stopwords\n",
        "df.text = df.text.apply(lambda text: remove_stopwords(text))\n",
        "df.text[:3].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aOruPLUWbYm"
      },
      "source": [
        "### Numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTDGL4YjWbYm"
      },
      "source": [
        "We may remove numbers if they are not useful in our analysis. But analysis in the financial domain, numbers are very useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "dkv3mTysWbYm"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.str.replace(r'\\d+', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0enKbnjuWbYm"
      },
      "source": [
        "### Extra whitespaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVnXE-urWbYm"
      },
      "source": [
        "After usually after preprocessing the text there might be extra whitespaces that might be created after transforming, removing various characters. Also, there is a need to remove all the new line, tab characters as well from our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8rnB6zmUWbYm"
      },
      "outputs": [],
      "source": [
        "def remove_whitespaces(text):\n",
        "    return # put your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "V_EbkbKMWbYn",
        "outputId": "c6e2b767-17a9-4c57-fbfd-814121f9b74d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Whitespaces in the beginning are removed as well as in between the text'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"  Whitespaces in the beginning are removed  \\t as well \\n  as in between  the text   \"\n",
        "\n",
        "clean_text = \" \".join(text.split())\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bxMmao2fWbYn"
      },
      "outputs": [],
      "source": [
        "df.text = df.text.apply(lambda x: remove_whitespaces(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2DhSwhSWbYn"
      },
      "source": [
        "### Frequent words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaGQr1G7WbYn"
      },
      "source": [
        "Previously we have removed stopwords which are common in any language. If we are working in any domain, we can also remove the common words used in that domain which don't provide us with much information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Qntsj4h7WbYn"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def freq_words(text):\n",
        "    # put your code here\n",
        "    return FrequentWords\n",
        "\n",
        "def remove_fw(text, FrequentWords):\n",
        "    # put your code here\n",
        "    return without_fw\n",
        "\n",
        "counter = Counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "a35_bpkjWbYn"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Natural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZZN277mTWbYn",
        "outputId": "ca8cbd66-ec2b-4824-e270-6d2d59f076c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[',', 'to', '.', 'is', 'the', 'understand', 'Natural', 'Language', 'Processing', 'computers']\n"
          ]
        }
      ],
      "source": [
        "FrequentWords = freq_words(text)\n",
        "print(FrequentWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "m7GRHuQMWbYn",
        "outputId": "28479437-cbec-4580-b260-e473d62cb112"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'technology used aid human ‚Äô s natural language It ‚Äô s not an easy task teaching machines how we communicate Leand Romaf an experienced software engineer who passionate at teaching people how artificial intelligence systems work says that ‚Äú in recent years there have been significant breakthroughs in empowering language just as we do. ‚Äù This article will give a simple introduction and how it can be achieved usually shortened as NLP a branch of artificial intelligence that deals with interaction between and humans using natural language The ultimate objective of NLP read decipher and make sense of human languages in a manner that valuable Most NLP techniques rely on machine learning derive meaning from human languages'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fw_result = remove_fw(text, FrequentWords)\n",
        "fw_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCdQDapHWbYo"
      },
      "source": [
        "### Rare words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhVqcAEFWbYo"
      },
      "source": [
        "Rare words are similar to frequent words. We can remove them because they are so less that they cannot add any value to the purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kD6c17O_WbYo"
      },
      "outputs": [],
      "source": [
        "def rare_words(text):\n",
        "    # put your code here\n",
        "\n",
        "    return RareWords\n",
        "\n",
        "def remove_rw(text, RareWords):\n",
        "    # put your code here\n",
        "    return without_rw\n",
        "\n",
        "counter = Counter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wrk1Tf0bWbYo"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Natural Language Processing is the technology used to aid computers to understand the human‚Äôs natural language. It‚Äôs not an easy task teaching machines to understand how we communicate. Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that ‚Äúin recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved. Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "TUSYh0ZkWbYo",
        "outputId": "0a49780e-80f7-40e6-da74-1b669670be90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['from', 'meaning', 'derive', 'learning', 'machine', 'on', 'rely', 'techniques', 'Most']\n"
          ]
        }
      ],
      "source": [
        "RareWords = rare_words(text)\n",
        "print(RareWords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "M4RpC7kLWbYo",
        "outputId": "4332a383-6c58-466f-8834-5ef7a99cea9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Natural Language Processing is the technology used to aid computers to understand the human ‚Äô s natural language . It ‚Äô s not an easy task teaching machines to understand how we communicate . Leand Romaf , an experienced software engineer who is passionate at teaching people how artificial intelligence systems work , says that ‚Äú in recent years , there have been significant breakthroughs in empowering computers to understand language just as we do. ‚Äù This article will give a simple introduction to Natural Language Processing and how it can be achieved . Natural Language Processing , usually shortened as NLP , is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language . The ultimate objective of NLP is to read , decipher , understand , and make sense of the human languages in a manner that is valuable . NLP to human languages .'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rw_result = remove_fw(text, RareWords)\n",
        "rw_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpClhcUIWbYo"
      },
      "source": [
        "## Conversion of Emoji to Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "F0WTiwNxWbYo",
        "outputId": "632351a5-48f7-4736-91f2-ff0ec4ae78cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/98/_przjd655416vf04xccddy280000gn/T/ipykernel_82587/2299328559.py:1: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ],
      "source": [
        "demoji.download_codes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "LdUuFQ7wWbYo"
      },
      "outputs": [],
      "source": [
        "def emoji_to_words(text):\n",
        "    return demoji.replace_with_desc(text, sep=\"__\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "CzoHlZUwWbYo",
        "outputId": "11ce32fb-b1c7-4ef2-dff3-5a4b44f903cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'game is on __fire__ __person rowing boat: medium-light skin tone__'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"game is on üî• üö£üèº\"\n",
        "emoji_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEIRf3EDWbYo"
      },
      "source": [
        "## Conversion of Emoticons to Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6ODdPMLWbYo"
      },
      "source": [
        "As we did for emoji's, we convert emoticons to words for the same purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Ik38fNZbWbYo"
      },
      "outputs": [],
      "source": [
        "def emoticons_to_words(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "M1cfeMfiWbYp",
        "outputId": "9bc81bd4-7911-4d9f-c489-785c91ec6327"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hey there!! Happy_face_smiley'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Hey there!! :-)\"\n",
        "emoticons_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86s95u8WbYp"
      },
      "source": [
        "## Converting Numbers to Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c88qFk3WbYp"
      },
      "source": [
        "If our analysis require us to use information based on the numbers in the text, we can convert them to words.\n",
        "\n",
        "Read more about num2words on [github](https://github.com/savoirfairelinux/num2words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "5QF84qwQWbYp"
      },
      "outputs": [],
      "source": [
        "def nums_to_words(text):\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "        if word.isdigit():\n",
        "            new_text.append(num2words(word))\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hmIDNBVfWbYr",
        "outputId": "b5b82991-fb98-4e92-e2ce-6d7315726409"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I ran this track thirty times'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I ran this track 30 times\"\n",
        "nums_to_words(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scjWWVNCWbYr"
      },
      "source": [
        "## Chat words Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Vxu0wk80WbYr"
      },
      "outputs": [],
      "source": [
        "chat_words = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "QPSA?=Que Pasa?\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "OMG=Oh my god\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "BipFm_2zWbYs"
      },
      "outputs": [],
      "source": [
        "chat_words_dict = dict()\n",
        "chat_words_set = set()\n",
        "\n",
        "def cw_conversion(text):\n",
        "    new_text = []\n",
        "    for word in text.split():\n",
        "        if word.upper() in chat_words_set:\n",
        "            new_text.append(chat_words_dict[word.upper()])\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "for line in chat_words.split('\\n'):\n",
        "    if line != '':\n",
        "        cw, cw_expanded = line.split('=')[0], line.split('=')[1]\n",
        "        \n",
        "        chat_words_set.add(cw)\n",
        "        chat_words_dict[cw] = cw_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "wUp407f-WbYs",
        "outputId": "517f9b98-5f40-4c31-d495-88d4e4787e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Oh my god that's awesome.\""
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"omg that's awesome.\"\n",
        "cw_conversion(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNp5g9DWbYs"
      },
      "source": [
        "## Expanding Contractions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMd2fuliWbYs"
      },
      "source": [
        "Contractions are words or combinations of words created by dropping a few letters and replacing those letters by an apostrophe.\n",
        "\n",
        "Example:\n",
        "- don't: do not\n",
        "- we'll: we will\n",
        "\n",
        "Our nlp model don't understand these contractions i.e. they don't understand that \"don't\" and \"do not\" are the same thing. If our problem statement requires them then we can expand them or else leave it as it is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "fXfZi67EWbYs"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    expanded_text = []\n",
        "    for line in text:\n",
        "        expanded_text.append(contractions.fix(line))\n",
        "    return expanded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KCx1gIp1WbYs",
        "outputId": "0b04dd5a-5576-4ee1-c251-ca5a44db8c49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I will be there within 15 minutes.',\n",
              " 'It is awesome to meet your new friends.']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = [\"I'll be there within 15 minutes.\", \"It's awesome to meet your new friends.\"]\n",
        "expand_contractions(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDAod5SeWbYs"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "OWaTh1j6WbYs"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    # put your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "0HvlxNLyWbYs",
        "outputId": "bcba4e01-77ad-4845-fd10-6cebd1d53316"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>use chrome</td>\n",
              "      <td>use chrome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>manga cute yet made cry</td>\n",
              "      <td>manga cute yet made cri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>goddess</td>\n",
              "      <td>goddess</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amp tf cant find subtitles oitnb need understand changs backstory</td>\n",
              "      <td>amp tf cant find subtitl oitnb need understand chang backstori</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>know na negotiate bride price advance payment</td>\n",
              "      <td>know na negoti bride price advanc payment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                text  \\\n",
              "0                                                         use chrome   \n",
              "1                                            manga cute yet made cry   \n",
              "2                                                            goddess   \n",
              "3  amp tf cant find subtitles oitnb need understand changs backstory   \n",
              "4                      know na negotiate bride price advance payment   \n",
              "\n",
              "                                                     text_stemmed  \n",
              "0                                                      use chrome  \n",
              "1                                         manga cute yet made cri  \n",
              "2                                                         goddess  \n",
              "3  amp tf cant find subtitl oitnb need understand chang backstori  \n",
              "4                       know na negoti bride price advanc payment  "
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text_stemmed'] = df.text.apply(lambda text: stem_words(text))\n",
        "df[['text', 'text_stemmed']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG1wtbucWbYs"
      },
      "source": [
        "PorterStemmer can be used only for english. If we are working with other than english then we can use SnowballStemmer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "Kr45cNnDWbYs",
        "outputId": "4bb0c6ee-5603-4ed5-e163-33a33cb8bb90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SnowballStemmer.languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAeojpwbWbYt"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Bnqzl__4WbYt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/yj.lee/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/yj.lee/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def text_lemmatize(text):\n",
        "    # put your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "KMDQXduMWbYt",
        "outputId": "f728a27b-d632-499c-c715-84e31a0e2653"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_stemmed</th>\n",
              "      <th>text_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>use chrome</td>\n",
              "      <td>use chrome</td>\n",
              "      <td>use chrome</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>manga cute yet made cry</td>\n",
              "      <td>manga cute yet made cri</td>\n",
              "      <td>manga cute yet made cry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>goddess</td>\n",
              "      <td>goddess</td>\n",
              "      <td>goddess</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>amp tf cant find subtitles oitnb need understand changs backstory</td>\n",
              "      <td>amp tf cant find subtitl oitnb need understand chang backstori</td>\n",
              "      <td>amp tf cant find subtitle oitnb need understand chang backstory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>know na negotiate bride price advance payment</td>\n",
              "      <td>know na negoti bride price advanc payment</td>\n",
              "      <td>know na negotiate bride price advance payment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                text  \\\n",
              "0                                                         use chrome   \n",
              "1                                            manga cute yet made cry   \n",
              "2                                                            goddess   \n",
              "3  amp tf cant find subtitles oitnb need understand changs backstory   \n",
              "4                      know na negotiate bride price advance payment   \n",
              "\n",
              "                                                     text_stemmed  \\\n",
              "0                                                      use chrome   \n",
              "1                                         manga cute yet made cri   \n",
              "2                                                         goddess   \n",
              "3  amp tf cant find subtitl oitnb need understand chang backstori   \n",
              "4                       know na negoti bride price advanc payment   \n",
              "\n",
              "                                                   text_lemmatized  \n",
              "0                                                       use chrome  \n",
              "1                                          manga cute yet made cry  \n",
              "2                                                          goddess  \n",
              "3  amp tf cant find subtitle oitnb need understand chang backstory  \n",
              "4                    know na negotiate bride price advance payment  "
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['text_lemmatized'] = df.text.apply(lambda text: text_lemmatize(text))\n",
        "df[['text', 'text_stemmed', 'text_lemmatized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBP7Agt0WbYu"
      },
      "source": [
        "## Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RRJVrhfqWbYu"
      },
      "outputs": [],
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spelling(text):\n",
        "    correct_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            correct_text.append(spell.correction(word))\n",
        "        else:\n",
        "            correct_text.append(word)\n",
        "    return \" \".join(correct_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zR0yN2zdWbYu",
        "outputId": "86d8cf04-ea06-4ff2-f356-7e6b20a95606"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hi, how are you doing I'm good thanks for asking\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Hi, hwo are you doin? I'm good thnks for asking\"\n",
        "correct_spelling(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PYvCFqp3WbYv",
        "outputId": "58db5d8f-0654-4854-bcc9-00b495c2f62e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"he are you doing I'm god thanks\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"hw are you doin? I'm god thnks\"\n",
        "correct_spelling(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r956E3YqWbYw"
      },
      "source": [
        "## Convert accented characters to ASCII characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "0U6ON74VWbYw"
      },
      "outputs": [],
      "source": [
        "def accented_to_ascii(text):\n",
        "    return unidecode.unidecode(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "BoICkLeyWbYx",
        "outputId": "a58f6873-78c5-4dcf-9e85-1518acf13c28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'This is an example text with accented characters like deep learning and computer vision etc.'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"This is an example text with accented characters like d√®√®p l√®arning √°nd c√∂mputer v√≠s√≠√∂n etc.\"\n",
        "accented_to_ascii(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "2021-04-11-Text-Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
